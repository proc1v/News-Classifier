{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping evaluation using Gdelt v2 Events URLs (both EN + ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "import scrapy\n",
    "import warnings\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import plotly_express as px\n",
    "import scrapy.crawler as crawler\n",
    "\n",
    "from functools import partial\n",
    "from newspaper import Article\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from multiprocessing import Process, Queue\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "nltk.download('punkt')\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "SPACE_REGEX = re.compile(r\"\\s+\")\n",
    "REGEX_TOKENIZER = re.compile(r'\\w+')\n",
    "LAT_LONG_REGEX = re.compile(r\"[\\#,]\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Path to data, models, results\n",
    "# ==================================\n",
    "HOME_DIR = os.environ['HOME_PROJECT_X'] if 'HOME_PROJECT_X' in os.environ else r'C:\\ProjectX'\n",
    "\n",
    "WORKSPACE_ROOT = os.path.join(HOME_DIR, 'workspace')\n",
    "PATH_TO_PROJECT_X_REPO = os.path.join(WORKSPACE_ROOT, 'project_x')\n",
    "PATH_TO_DATA_ROOT_DIR = os.path.join(WORKSPACE_ROOT, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to library to sys path\n",
    "generic_utils_lib_dir = os.path.join(PATH_TO_PROJECT_X_REPO, 'common')\n",
    "\n",
    "sys.path.extend([generic_utils_lib_dir])\n",
    "\n",
    "from generic_utils import (downcast_datatypes, timing, create_output_dir, parallelize, create_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNICODE_REGEX = re.compile(r'[^\\x00-\\x7F]+', re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_article(url, verbose=False):\n",
    "    \n",
    "    if verbose:\n",
    "        print(url)\n",
    "    \n",
    "    article = Article(url) \n",
    "    article.download()   \n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Authors: {article.authors}\")\n",
    "        print(f\"Publish date: {article.publish_date}\\n\")\n",
    "    \n",
    "        print(\"=\" * 40)\n",
    "        print(article.text)\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        print(f\"Keywords: {article.keywords}\")\n",
    "        print(f\"Summary: {article.summary}\")\n",
    "    \n",
    "    return article.text\n",
    "\n",
    "\n",
    "def squash_spaces(s, space_re=SPACE_REGEX) -> str:\n",
    "    return re.sub(space_re, \" \", s) if isinstance(s, str) else s\n",
    "\n",
    "\n",
    "def preprocess_body_text(text, normalize=True):\n",
    "    \n",
    "    if text:\n",
    "        # Replacing possible issues with data. We can add or reduce the replacement in this chain\n",
    "        s = re.sub(r'\\n+', ' ', str(text))\n",
    "        s = squash_spaces(s).strip()\n",
    "\n",
    "        if normalize:\n",
    "            # Normalizing / encoding the text\n",
    "            s = unidecode.unidecode(s)\n",
    "        \n",
    "        return s\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_lat_long(x):\n",
    "    if isinstance(x, str):\n",
    "        return float(re.sub(LAT_LONG_REGEX, \"\", x))\n",
    "    return x\n",
    "    \n",
    "\n",
    "def line_contains_only_digits(x):\n",
    "    return x.isdecimal() if isinstance(x, str) else True\n",
    "\n",
    "\n",
    "def line_contain_chars(l):\n",
    "    return any(re.findall(\"[A-Za-z]+\", l)) if isinstance(l, str) else False\n",
    "\n",
    "\n",
    "def line_contains_unicode(l):\n",
    "    return any(UNICODE_REGEX.findall(l)) if isinstance(l, str) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_error_codes = {\n",
    "    '100': 'Informational - Continue',\n",
    "    '101': 'Informational - Switching Protocols',\n",
    "    '200': 'Successful - OK',\n",
    "    '201': 'Successful - Created',\n",
    "    '202': 'Successful - Accepted',\n",
    "    '203': 'Successful - Non-Authoritative Information',\n",
    "    '204': 'Successful - No Content',\n",
    "    '205': 'Successful - Reset Content',\n",
    "    '206': 'Successful - Partial Content',\n",
    "    '300': 'Redirection - Multiple Choices',\n",
    "    '301': 'Redirection - Moved Permanently',\n",
    "    '302': 'Redirection - Found',\n",
    "    '303': 'Redirection - See Other',\n",
    "    '304': 'Redirection - Not Modified',\n",
    "    '305': 'Redirection - Use Proxy',\n",
    "    '307': 'Redirection - Temporary Redirect',\n",
    "    '400': 'Client Error - Bad Request',\n",
    "    '401': 'Client Error - Unauthorized',\n",
    "    '402': 'Client Error - Payment Required',\n",
    "    '403': 'Client Error - Forbidden',\n",
    "    '404': 'Client Error - Not Found',\n",
    "    '405': 'Client Error - Method Not Allowed',\n",
    "    '406': 'Client Error - Not Acceptable',\n",
    "    '407': 'Client Error - Proxy Authentication Required',\n",
    "    '408': 'Client Error - Request Timeout',\n",
    "    '409': 'Client Error - Conflict',\n",
    "    '410': 'Client Error - Gone',\n",
    "    '411': 'Client Error - Length Required',\n",
    "    '412': 'Client Error - Precondition Failed',\n",
    "    '413': 'Client Error - Request Entity Too Large',\n",
    "    '414': 'Client Error - Request-URI Too Long',\n",
    "    '415': 'Client Error - Unsupported Media Type',\n",
    "    '416': 'Client Error - Requested Range Not Satisfiable',\n",
    "    '417': 'Client Error - Expectation Failed',\n",
    "    '500': 'Server Error - Internal Server Error',\n",
    "    '501': 'Server Error - Not Implemented',\n",
    "    '502': 'Server Error - Bad Gateway',\n",
    "    '503': 'Server Error - Service Unavailable',\n",
    "    '504': 'Server Error - Gateway Timeout',\n",
    "    '505': 'Server Error - HTTP Version Not Supported',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Loading unique URLs from GDELT v2 Events EN + ML feeds\n",
    "---\n",
    "\n",
    "Data has been prepared in gdelt_v2_events_data_eda.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime('2020-01-01 00:00:00')\n",
    "end = pd.to_datetime('2022-01-20 09:00:00')\n",
    "\n",
    "path_to_gdelt = os.path.join(PATH_TO_DATA_ROOT_DIR, 'data_providers/gdelt')\n",
    "path_to_scraped_data = os.path.join(path_to_gdelt, 'scraped_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'events_v02_en_ml_urls_{str(start.date())}_{str(end.date())}.parquet'\n",
    "\n",
    "print(f\"Loading GDELT v2.0 unique URLs from {os.path.join(path_to_gdelt, fn)}\")\n",
    "events_v02_grouped = pd.read_parquet(os.path.join(path_to_gdelt, fn),\n",
    "                                     engine='auto',\n",
    "                                     columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's filter to more recent URLs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_urls_recent = events_v02_grouped.loc[\n",
    "    events_v02_grouped['dateadded_min'] > pd.to_datetime('2021-09-01 09:00:00'), 'sourceurl'].values\n",
    "\n",
    "print(len(unique_urls_recent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(18)\n",
    "idx = np.random.random_integers(0, len(unique_urls_recent), 200000)\n",
    "URLS = list(unique_urls_recent[idx])\n",
    "\n",
    "# URLS = ['https://www.msn.com/en-us/news/world/enhancing-the-potential-of-people-with-disabilities/ar-AARJaWx']\n",
    "\n",
    "## Example of URLs with different errors\n",
    "\n",
    "# URLS = [\n",
    "#     'https://www.insurancejournal.com/jobs/633146-inside-represented-moderate-casualty-adjuster-remote',\n",
    "#     'https://omaha.com/news/national/mixed-feelings-in-el-paso-about-looser-texas-gun-limits/article_9fc209fc-6084-59de-a1f9-8987e7789fce.html',\n",
    "#     'https://www.bizpacreview.com/2021/11/13/close-biden-ally-implies-president-may-not-seek-second-term-for-whatever-reason-boosts-kamala-1162706/',\n",
    "#     'https://omaha.com/news/national/mixed-feelings-in-el-paso-about-looser-texas-gun-limits/article_9fc209fc-6084-59de-a1f9-8987e7789fce.html',\n",
    "#     'http://www.independent.com.mt/articles/2021-09-25/blogs-opinions/Strong-words-in-September-6736236989',\n",
    "#     'https://indiankanoon.org/doc/112114664/'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Scraping\n",
    "---\n",
    "\n",
    "It is based on original implementation by Jad (see https://github.com/BaseOperations/data-automation/blob/master/news_index/English-version/media_scraper/parse_text.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSpider(scrapy.Spider):\n",
    "    name = \"news_collection\"\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'\n",
    "    handle_httpstatus_list = [100, 101, 200, 201, 202, 203, 204, 205, 206, 300, 301, 302, 303, 304, 305, 307, 400, 401, 402,\n",
    "                              403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 500, 501, 502, 503, \n",
    "                              504, 505]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        super(NewsSpider, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.start_urls = URLS\n",
    "        \n",
    "        self.path_to_scraped_data = path_to_scraped_data\n",
    "        create_output_dir(self.path_to_scraped_data)\n",
    "        \n",
    "        self.filename = 'scraped_gdelt.csv'\n",
    "        \n",
    "        meta = {'download_timeout': 10}\n",
    "\n",
    "    def parse(self, response, verbose=False):\n",
    "        \n",
    "        url = response.request.url\n",
    "        status = response.status\n",
    "            \n",
    "        if status in [200, 201, 202, 203, 204, 205, 206]:\n",
    "            \n",
    "            tags = response.xpath(\"//meta[@property]\").extract()\n",
    "            final_tags = dict()\n",
    "            for tag in tags:\n",
    "\n",
    "                if len(re.findall(\"og:\", tag)) > 0:\n",
    "\n",
    "                    if re.search(\"property=\\\"og:\", tag) is None or re.search(\"content=\\\"\", tag) is None:\n",
    "                        continue\n",
    "\n",
    "                    prop_first = re.search(\"property=\\\"og:\", tag).span()[1]\n",
    "                    cont = re.search(\"content=\\\"\", tag).span()[1]\n",
    "                    prop_end = re.search(\"content=\\\"\", tag).span()[0] - 2\n",
    "\n",
    "                    cont_first = cont \n",
    "                    cont_end = len(tag) - 2\n",
    "\n",
    "                    proper = tag[prop_first:prop_end]\n",
    "                    content = tag[cont_first:cont_end]\n",
    "                    final_tags[proper] = str(content)\n",
    "\n",
    "            if response.xpath('//article'):\n",
    "                paragraphs = response.xpath('//article').css('p::text').getall()\n",
    "            else:\n",
    "                paragraphs = response.css('p::text').getall()\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\\n======================\")\n",
    "                print(f\"URL: {url}\")\n",
    "                print(f\"Status: {status}\")\n",
    "                print(f\"Tags: {final_tags}\")\n",
    "                print(f\"Body: {paragraphs}\")\n",
    "\n",
    "        else:\n",
    "            final_tags = {}\n",
    "            paragraphs = []\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"\\n======================\")\n",
    "                print(f\"URL: {url}\")\n",
    "                print(f\"Status: {status}\")\n",
    "                print(f\"Tags: {final_tags}\")\n",
    "                print(f\"Body: {paragraphs}\")\n",
    "\n",
    "            \n",
    "        with open(os.path.join(self.path_to_scraped_data, self.filename), 'a', newline='') as myfile:\n",
    "            writer = csv.writer(myfile)\n",
    "            writer.writerow([url, status, paragraphs, final_tags])\n",
    "\n",
    "def run_spider(spider):\n",
    "    \"\"\"\n",
    "    The wrapper to make it run more times\n",
    "    \"\"\"\n",
    "    def f(q):\n",
    "        try:\n",
    "            runner = crawler.CrawlerRunner()\n",
    "            deferred = runner.crawl(spider)\n",
    "            deferred.addBoth(lambda _: reactor.stop())\n",
    "            reactor.run()\n",
    "            q.put(None)\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "    \n",
    "    if result is not None:\n",
    "        raise result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_scraping = False\n",
    "\n",
    "if run_scraping:\n",
    "    run_spider(NewsSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1000 - 1min 15s\n",
    "- 10000 - 23 min\n",
    "- 200000 - approx 7h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading scraped URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'scraped_gdelt.csv'\n",
    "\n",
    "scraping_analysis = pd.read_csv(os.path.join(path_to_scraped_data, filename), header=None,\n",
    "                                names=['url', 'status', 'paragraphs', 'final_tags'])\n",
    "\n",
    "scraping_analysis.drop_duplicates(subset=['url'], inplace=True)\n",
    "\n",
    "scraping_analysis['error_code'] = scraping_analysis['status'].astype(str) \\\n",
    "                                  + ' - ' \\\n",
    "                                  + scraping_analysis['status'].astype(str).map(http_error_codes)\n",
    "\n",
    "scraping_analysis['paragraphs'] = scraping_analysis['paragraphs'].map(lambda x: eval(x) if x else np.nan)\n",
    "mask = scraping_analysis['paragraphs'].notnull()\n",
    "scraping_analysis.loc[mask, 'paragraphs'] = scraping_analysis.loc[mask, 'paragraphs'].map(lambda x: ' '.join(x).strip())\n",
    "\n",
    "# Apply light preprocessing\n",
    "preprocess_body_news_simple = partial(preprocess_body_text, normalize=False)\n",
    "scraping_analysis['paragraphs'] = scraping_analysis['paragraphs'].apply(preprocess_body_news_simple)\n",
    "scraping_analysis['paragraphs'] = scraping_analysis['paragraphs'].replace({'': np.nan, '❌': np.nan}) \n",
    "\n",
    "# Len of news body\n",
    "mask = scraping_analysis['paragraphs'].notnull()\n",
    "# scraping_analysis.loc[mask, 'paragraphs_nchars'] = scraping_analysis.loc[mask, 'paragraphs'].str.len()\n",
    "scraping_analysis.loc[mask, 'paragraphs_nwords'] = scraping_analysis.loc[mask, 'paragraphs'].str.split().apply(len)\n",
    "\n",
    "# Adding source name\n",
    "scraping_analysis['source_name'] = \\\n",
    "    scraping_analysis['url'].str.split('://').str[1].str.split('/').str[0].str.split(\"\\?|\\:\").str[0]\n",
    "\n",
    "cols_order = [\n",
    "    'source_name', 'url', 'status', 'paragraphs', 'final_tags', 'error_code', 'paragraphs_nwords'\n",
    "]\n",
    "scraping_analysis = scraping_analysis[cols_order]\n",
    "\n",
    "print(f\"Number of news scraped: {scraping_analysis.shape[0]}\")\n",
    "\n",
    "print(f\"PCT of news with no text (prior actually nullifying news with garbage text content): \"\n",
    "      f\"{scraping_analysis['paragraphs'].isnull().sum() / scraping_analysis.shape[0]} \")\n",
    "\n",
    "print(f\"Number of unique sources covered: {scraping_analysis['source_name'].nunique()}\")\n",
    "\n",
    "scraping_analysis.sample(5, random_state=2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_analysis['error_code'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likely problematic scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = scraping_analysis.groupby('paragraphs').agg(\n",
    "    {'url': ['count', 'nunique', set],\n",
    "     'source_name': ['nunique', set],\n",
    "     'paragraphs_nwords': ['first']}).reset_index()\n",
    "\n",
    "grouped.columns = [\"_\".join(filter(lambda col: col, col)) for col in grouped.columns.ravel()]\n",
    "grouped = grouped[grouped['url_count'] > 1].sort_values('url_count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Likely garbage text in news' body\n",
    "likely_scraper_problem = grouped['paragraphs'].tolist()\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_analysis_filtered = scraping_analysis.copy()\n",
    "\n",
    "mask = scraping_analysis_filtered['paragraphs'].isin(likely_scraper_problem)\n",
    "print(f\"Number of likely problematic scraping: {scraping_analysis_filtered[mask].shape}\")\n",
    "\n",
    "scraping_analysis_filtered.loc[mask, 'paragraphs'] = np.nan\n",
    "scraping_analysis_filtered.loc[mask, 'paragraphs_nwords'] = np.nan\n",
    "scraping_analysis_filtered.loc[mask, 'paragraphs_nwords_bins'] = np.nan\n",
    "\n",
    "print(scraping_analysis_filtered['paragraphs'].isnull().sum() / scraping_analysis_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = scraping_analysis\n",
    "df = scraping_analysis_filtered\n",
    "\n",
    "mask = df['paragraphs_nwords'] <= 200\n",
    "\n",
    "print(df.loc[mask].shape)\n",
    "df.loc[mask, 'paragraphs'].value_counts().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = scraping_analysis\n",
    "df = scraping_analysis_filtered\n",
    "\n",
    "news_text_stats = df['paragraphs_nwords'].value_counts().reset_index().rename(\n",
    "    columns={'index': 'paragraphs_nwords', 'paragraphs_nwords': 'count'}\n",
    ")\n",
    "\n",
    "n_tokens_bins = \\\n",
    "    [b for b in [0, 5, 10, 50, 100, 250, 500, 1000, 2000, 5000] if b <= news_text_stats['paragraphs_nwords'].max()] + \\\n",
    "    [news_text_stats['paragraphs_nwords'].max()]\n",
    "\n",
    "news_text_stats['paragraphs_nwords_bins'] = pd.cut(news_text_stats['paragraphs_nwords'], \n",
    "                                                   n_tokens_bins, retbins=True, precision=1)[0]\n",
    "\n",
    "grouped = news_text_stats.groupby('paragraphs_nwords_bins').agg({'count': ['sum']}).reset_index()\n",
    "grouped.columns = [\"_\".join(filter(lambda col: col, col)) for col in grouped.columns.ravel()]\n",
    "grouped.rename(columns={'count_sum': 'number_of_records'}, inplace=True)\n",
    "\n",
    "grouped['pct_of_records'] = (grouped['number_of_records'] / news_text_stats['count'].sum()).round(2)\n",
    "grouped['paragraphs_nwords_bins'] = grouped['paragraphs_nwords_bins'].astype(str)\n",
    "\n",
    "fig = px.bar(grouped, x='paragraphs_nwords_bins', y='pct_of_records', height=350,\n",
    "             text='pct_of_records', \n",
    "             title=f'PCT of records having number of words in news within the corresponding bin')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = scraping_analysis_filtered['paragraphs_nwords'] <= 20\n",
    "\n",
    "print(scraping_analysis_filtered[mask].shape)\n",
    "\n",
    "scraping_analysis_filtered[mask].sort_values('paragraphs_nwords').head(200)#['paragraphs'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Appendix - original code of Jad to scrape news\n",
    "- doesn't work in Jupyter environment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class news_collection(scrapy.Spider):\n",
    "    name = \"news_collection\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(news_collection, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = kwargs.get(\"urls\")\n",
    "        meta = {'download_timeout': 10}\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "    \n",
    "        tags = response.xpath(\"//meta[@property]\").extract()\n",
    "        final_tags = dict()\n",
    "        for tag in tags:\n",
    "            \n",
    "            if len(re.findall(\"og:\", tag)) > 0:\n",
    "                \n",
    "                if re.search(\"property=\\\"og:\", tag) is None or re.search(\"content=\\\"\", tag) is None:\n",
    "                    continue\n",
    "                    \n",
    "                prop_first = re.search(\"property=\\\"og:\", tag).span()[1]\n",
    "                cont = re.search(\"content=\\\"\", tag).span()[1]\n",
    "                prop_end = re.search(\"content=\\\"\", tag).span()[0] - 2\n",
    "                \n",
    "                cont_first = cont \n",
    "                cont_end = len(tag)-2\n",
    "                \n",
    "                proper = tag[prop_first:prop_end]\n",
    "                content = tag[cont_first:cont_end]\n",
    "                final_tags[proper] = str(content)\n",
    "\n",
    "        if response.xpath('//article'):\n",
    "            paragraphs = response.xpath('//article').css('p::text').getall()\n",
    "        else:\n",
    "            paragraphs = response.css('p::text').getall()\n",
    "        \n",
    "        print(\"\\n======================\")\n",
    "        print(f\"Tags: {final_tags}\")\n",
    "        print(f\"Body: {paragraphs}\")\n",
    "        # print(f\"URL: {URL}\")\n",
    "        \n",
    "#         with open('/tmp/urls_list.json', 'r') as r:\n",
    "#             data = json.load(r)\n",
    "\n",
    "#         url = response.request.url\n",
    "#         for item in data:\n",
    "#             print(item)\n",
    "#             if item['URL'] == url:\n",
    "#                 item['Tags'] = final_tags\n",
    "#                 item['Body'] = paragraphs\n",
    "#                 item['URL'] = url\n",
    "#                 with open('/tmp/urls_list.json', 'w') as a:\n",
    "#                     json.dump(data, a, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scraper = False\n",
    "\n",
    "if run_scraper:\n",
    "    process = CrawlerProcess()\n",
    "    a = process.crawl(news_collection, urls=URLS)\n",
    "    process.start(stop_after_crawl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38dnn] *",
   "language": "python",
   "name": "conda-env-py38dnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
