{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching and processing Gdelt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 2 years of data - use m5.24xlagre\n",
    "- 3 mo of data - use 5.12xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import zipfile\n",
    "import warnings\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import plotly_express as px\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "nltk.download('punkt')\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "SPACE_REGEX = re.compile(r\"\\s+\")\n",
    "REGEX_TOKENIZER = re.compile(r'\\w+')\n",
    "LAT_LONG_REGEX = re.compile(r\"[\\#,]\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Path to data, models, results\n",
    "# ==================================\n",
    "HOME_DIR = os.environ['HOME_PROJECT_X'] if 'HOME_PROJECT_X' in os.environ else r'C:\\ProjectX'\n",
    "\n",
    "WORKSPACE_ROOT = os.path.join(HOME_DIR, 'workspace')\n",
    "PATH_TO_PROJECT_X_REPO = os.path.join(WORKSPACE_ROOT, 'project_x')\n",
    "PATH_TO_DATA_ROOT_DIR = os.path.join(WORKSPACE_ROOT, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to library to sys path\n",
    "generic_utils_lib_dir = os.path.join(PATH_TO_PROJECT_X_REPO, 'common')\n",
    "\n",
    "sys.path.extend([generic_utils_lib_dir])\n",
    "\n",
    "from generic_utils import (downcast_datatypes, timing, create_output_dir, parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_output_dir(PATH_TO_DATA_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_for_time_range(start: str, end: str, multilingua_data: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Inclusive datetime range\n",
    "    \"\"\"\n",
    "    if multilingua_data:\n",
    "        # Multilingual data\n",
    "        master_link = 'http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt' \n",
    "    else:\n",
    "        # EN data\n",
    "        master_link = 'http://data.gdeltproject.org/gdeltv2/masterfilelist.txt' \n",
    "    \n",
    "    get_links_to_processed_data_gdelt_v2 = pd.read_csv(master_link, header=None, sep='\\s', \n",
    "                                                       names=['id_1', 'id_2', 'url'])\n",
    "    \n",
    "    get_links_to_processed_data_gdelt_v2['feed_type'] = \\\n",
    "        get_links_to_processed_data_gdelt_v2['url'].str.split('.').str[-3] \n",
    "    \n",
    "    datetime_idx = -5 if multilingua_data else -4\n",
    "    \n",
    "    get_links_to_processed_data_gdelt_v2['datetime'] = \\\n",
    "        get_links_to_processed_data_gdelt_v2['url'].str.split('.').str[datetime_idx].str.split('/').str[-1]\n",
    "    \n",
    "    get_links_to_processed_data_gdelt_v2['datetime'] = \\\n",
    "        pd.to_datetime(get_links_to_processed_data_gdelt_v2['datetime'], format='%Y%m%d%H%M%S')\n",
    "    \n",
    "    print(f\"All Gdelt links (3 feeds: events, mentions, knowledge graph): \"\n",
    "          f\"{get_links_to_processed_data_gdelt_v2.shape[0]}\")\n",
    "    print(f\"- How many records per each feed: \"\n",
    "          f\"{get_links_to_processed_data_gdelt_v2['feed_type'].value_counts().to_dict()}\")\n",
    "\n",
    "    sample = get_links_to_processed_data_gdelt_v2[get_links_to_processed_data_gdelt_v2['datetime'].between(start, end)]\n",
    "    print(f\"- Number of Gdelt links for time range [{start}: {end}]: {sample.shape[0]}\\n\")\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "def fetch_events_data(gdelt_v02_events_link: str) -> pd.DataFrame:\n",
    "\n",
    "    use_cols_export = [\n",
    "        'GLOBALEVENTID', 'SQLDATE', 'EventCode', 'EventBaseCode', 'EventRootCode', 'GoldsteinScale',\n",
    "        'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_Lat', 'ActionGeo_Long', \n",
    "        'DATEADDED', 'SOURCEURL'\n",
    "    ]\n",
    "\n",
    "    cols_order = [\n",
    "        'globaleventid', 'sqldate', 'dateadded', 'eventrootcode', 'eventbasecode', 'eventcode', 'goldsteinscale', \n",
    "        'actiongeo_fullname', 'actiongeo_countrycode', 'actiongeo_countrycode_iso2', 'actiongeo_countrycode_iso3',\n",
    "        'actiongeo_lat', 'actiongeo_long', 'sourceurl'\n",
    "    ]\n",
    "   \n",
    "\n",
    "    try:\n",
    "        events_v02_export_df = pd.read_csv(gdelt_v02_events_link, sep='\\t', encoding = \"ISO-8859-1\", \n",
    "                                           header=None, names=event_export_header)\n",
    "        \n",
    "        events_v02_export_df = events_v02_export_df[use_cols_export]\n",
    "        events_v02_export_df.columns = events_v02_export_df.columns.str.lower()\n",
    "        events_v02_export_df['sqldate'] =  pd.to_datetime(events_v02_export_df['sqldate'], format='%Y%m%d')\n",
    "        events_v02_export_df['dateadded'] = pd.to_datetime(events_v02_export_df['dateadded'], format='%Y%m%d%H%M%S')\n",
    "        \n",
    "        events_v02_export_df['actiongeo_countrycode_iso2'] = \\\n",
    "            events_v02_export_df['actiongeo_countrycode'].map(map_fips_to_iso2)\n",
    "        \n",
    "        events_v02_export_df['actiongeo_countrycode_iso3'] = \\\n",
    "            ['actiongeo_countrycode_iso2'].map(map_iso2_to_iso3)\n",
    "        \n",
    "        events_v02_export_df = events_v02_export_df[cols_order]\n",
    "\n",
    "        # Fix issues in data\n",
    "        events_v02_export_df['actiongeo_lat'] = events_v02_export_df['actiongeo_lat'].apply(clean_lat_long)\n",
    "        events_v02_export_df['actiongeo_long'] = events_v02_export_df['actiongeo_long'].apply(clean_lat_long)\n",
    "    \n",
    "        for c in ['eventrootcode', 'eventbasecode', 'eventcode']:\n",
    "            events_v02_export_df[c] = pd.to_numeric(events_v02_export_df[c], errors='coerce')\n",
    "\n",
    "        for c in ['actiongeo_lat', 'actiongeo_long', 'goldsteinscale']:\n",
    "            events_v02_export_df[c] = pd.to_numeric(events_v02_export_df[c], errors='coerce')\n",
    "           \n",
    "        # Drop records with any NULL entries in the following columns\n",
    "        mask_drop_null_records = \\\n",
    "            events_v02_export_df[['eventrootcode', 'eventbasecode', 'eventcode', 'sourceurl']].isnull().any(axis=1)\n",
    "        \n",
    "        events_v02_export_df = events_v02_export_df[~mask_drop_null_records]\n",
    "        \n",
    "        # Convert data-types\n",
    "        for c in ['eventrootcode', 'eventbasecode', 'eventcode']:\n",
    "            events_v02_export_df[c] = events_v02_export_df[c].astype(np.int16)\n",
    "\n",
    "        for c in ['actiongeo_lat', 'actiongeo_long', 'goldsteinscale']:\n",
    "            events_v02_export_df[c] = events_v02_export_df[c].astype(np.float32)\n",
    "            \n",
    "        return events_v02_export_df, ''\n",
    "    \n",
    "    except:\n",
    "        return pd.DataFrame(), gdelt_v02_events_link\n",
    "    \n",
    "\n",
    "def __parallel_fetch_events(gdelt_v02_events_link_list):\n",
    "    \n",
    "    events_v02_export_df_list = []\n",
    "    links_with_errors_list = []\n",
    "    \n",
    "    for l in gdelt_v02_events_link_list:\n",
    "        df_, links_with_errors = fetch_events_data(gdelt_v02_events_link=l)\n",
    "    \n",
    "        if not df_.empty:\n",
    "            events_v02_export_df_list.append(df_)\n",
    "\n",
    "        if links_with_errors:\n",
    "            links_with_errors_list.append(links_with_errors)\n",
    "        \n",
    "    events_v02_export_df = pd.concat(events_v02_export_df_list, ignore_index=True)\n",
    "    return events_v02_export_df, links_with_errors_list \n",
    "\n",
    "\n",
    "def clean_lat_long(x):\n",
    "    if isinstance(x, str):\n",
    "        return float(re.sub(LAT_LONG_REGEX, \"\", x))\n",
    "    return x\n",
    "    \n",
    "\n",
    "def line_contains_only_digits(x):\n",
    "    return x.isdecimal() if isinstance(x, str) else True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# FIPS country codes to ISO2 (Gdelt uses old standard - FIPS 10-4)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_to_iso2 = pd.read_csv(os.path.join(PATH_TO_PROJECT_X_REPO, \"auxiliary_data\", \"countries_fips_to_iso2.csv\"), \n",
    "                           keep_default_na=False, na_values='NULL')\n",
    "\n",
    "fips_to_iso2 = fips_to_iso2.replace({'-': np.nan})\n",
    "\n",
    "mask_countries_in_iso2_not_in_fips = fips_to_iso2['fips_10_4'].isnull()\n",
    "print(f\"\\nCountries in ISO2/3 but not in FIPS 10-4:\\n\\n{fips_to_iso2[mask_countries_in_iso2_not_in_fips]}\")\n",
    "\n",
    "fips_to_iso2 = fips_to_iso2.dropna(subset=['fips_10_4'])\n",
    "map_fips_to_iso2 = fips_to_iso2.set_index('fips_10_4')['iso2'].to_dict()\n",
    "map_iso2_to_iso3 = fips_to_iso2.set_index('iso2')['iso3'].to_dict()\n",
    "map_iso2_to_country_name = fips_to_iso2.set_index('iso2')['country_name'].to_dict()\n",
    "\n",
    "print(fips_to_iso2.shape)\n",
    "fips_to_iso2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_to_iso2[fips_to_iso2.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. GDELT v2 EVENTS (downloading 2020-2022)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_export_header = [\n",
    "    'GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate', 'Actor1Code',\n",
    "    'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode',\n",
    "    'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code', 'Actor1Type2Code',\n",
    "    'Actor1Type3Code', 'Actor2Code', 'Actor2Name', 'Actor2CountryCode', \n",
    "    'Actor2KnownGroupCode', 'Actor2EthnicCode', 'Actor2Religion1Code', \n",
    "    'Actor2Religion2Code', 'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', \n",
    "    'IsRootEvent', 'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass', \n",
    "    'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone', \n",
    "    'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode', 'Actor1Geo_ADM1Code',\n",
    "    'Actor1Geo_ADM2Code', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'Actor1Geo_FeatureID', \n",
    "    'Actor2Geo_Type', 'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code',\n",
    "    'Actor2Geo_ADM2Code', 'Actor2Geo_Lat', 'Actor2Geo_Long', 'Actor2Geo_FeatureID', \n",
    "    'ActionGeo_Type', 'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code',\n",
    "    'ActionGeo_ADM2Code', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', \n",
    "    'DATEADDED', 'SOURCEURL'\n",
    "]\n",
    "\n",
    "gkg_header = [\n",
    "    'GKGRECORDID', 'DATE', 'SourceCollectionIdentifier', 'SourceCommonName', \n",
    "    'DocumentIdentifier', 'Counts', 'V2Counts', 'Themes', 'V2Themes', 'Locations', \n",
    "    'V2Locations', 'Persons', 'V2Persons', 'Organizations', 'V2Organizations', 'V2Tone', \n",
    "    'Dates', 'GCAM', 'SharingImage', 'RelatedImages', 'SocialImageEmbeds', \n",
    "    'SocialVideoEmbeds', 'Quotations', 'AllNames', 'Amounts', 'TranslationInfo',\n",
    "    'Extras'\n",
    "]\n",
    "\n",
    "cameo_event_root_code = {\n",
    "    '14': 'PROTESTS',\n",
    "    '18': 'ASSAULT', \n",
    "    '19': 'FIGHT',\n",
    "    '20': 'USE UNCONVENTIONAL MASS VIOLENCE'\n",
    "}\n",
    "\n",
    "cameo_event_base_code = {\n",
    "    '140': 'Engage in political dissent, not specified below',\n",
    "    '141': 'Demonstrate or rally, not specified below',\n",
    "    '142': 'Conduct hunger strike, not specified below',\n",
    "    '143': 'Conduct strike or boycott, not specified below',\n",
    "    '144': 'Obstruct passage, block, not specified below', \n",
    "    '145': 'Protest violently, riot, not specified below',\n",
    "\n",
    "    '180': 'Use unconventional violence, not specified below',\n",
    "    '181': 'Abduct, hijack, or take hostage',\n",
    "    '182': 'Physically assault, not specified below',\n",
    "    '183': 'Conduct suicide, car, or other non-military bombing, not specified below',\n",
    "    '184': 'Use as human shield',\n",
    "    '185': 'Attempt to assassinate',\n",
    "    '186': 'Assassinate',\n",
    "\n",
    "    '190': 'Use conventional military force, not specified below',\n",
    "    '191': 'Impose blockade, restrict movement',\n",
    "    '192': 'Occupy territory',\n",
    "    '193': 'Fight with small arms and light weapons',\n",
    "    '194': 'Fight with artillery and tanks',\n",
    "    '195': 'Employ aerial weapons, not specified below',\n",
    "    '196': 'Violate ceasefire',\n",
    "\n",
    "    '200': 'Use unconventional mass violence, not specified below',\n",
    "    '201': 'Engage in mass expulsion',\n",
    "    '202': 'Engage in mass killings',\n",
    "    '203': 'Engage in ethnic cleansing',\n",
    "    '204': 'Use weapons of mass destruction, not specified below' \n",
    "}\n",
    "\n",
    "cameo_event_code = {\n",
    "    '140': 'Engage in political dissent, not specified below',\n",
    "    '141': 'Demonstrate or rally, not specified below',\n",
    "    '1411': 'Demonstrate for leadership change',\n",
    "    '1412': 'Demonstrate for policy change',\n",
    "    '1413': 'Demonstrate for rights',\n",
    "    '1414': 'Demonstrate for change in institutions, regime', \n",
    "    '142': 'Conduct hunger strike, not specified below',\n",
    "    '1421': 'Conduct hunger strike for leadership change',  \n",
    "    '1422': 'Conduct hunger strike for policy change',  \n",
    "    '1423': 'Conduct hunger strike for rights',  \n",
    "    '1424': 'Conduct hunger strike for change in institutions, regime',  \n",
    "    \n",
    "    '143': 'Conduct strike or boycott, not specified below',\n",
    "    '1431': 'Conduct strike or boycott for leadership change',  \n",
    "    '1432': 'Conduct strike or boycott for policy change',  \n",
    "    '1433': 'Conduct strike or boycott for rights',  \n",
    "    '1434': 'Conduct strike or boycott for change in institutions, regime',  \n",
    "    \n",
    "    '144': 'Obstruct passage, block, not specified below', \n",
    "    '1441': 'Obstruct passage to demand leadership change',  \n",
    "    '1442': 'Obstruct passage to demand policy change', \n",
    "    '1443': 'Obstruct passage to demand rights', \n",
    "    '1444': 'Obstruct passage to demand change in institutions, regime',  \n",
    "    \n",
    "    '145': 'Protest violently, riot, not specified below',\n",
    "    '1451': 'Engage in violent protest for leadership change',  \n",
    "    '1452': 'Engage in violent protest for policy change',  \n",
    "    '1453': 'Engage in violent protest for rights',  \n",
    "    '1454': 'Engage in violent protest for change in institutions, regime', \n",
    "    \n",
    "    '180': 'Use unconventional violence, not specified below',\n",
    "    '181': 'Abduct, hijack, or take hostage',\n",
    "    '182': 'Physically assault, not specified below',\n",
    "    '1821': 'Sexually assault', \n",
    "    '1822': 'Torture', \n",
    "    '1823': 'Kill by physical assault', \n",
    "    \n",
    "    '183': 'Conduct suicide, car, or other non-military bombing, not specified below',\n",
    "    '1831': 'Carry out suicide bombing', \n",
    "    '1832': 'Carry out vehicular bombing', \n",
    "    '1833': 'Carry out roadside bombing', \n",
    "    '1834': 'Carry out location bombing', \n",
    "    \n",
    "    '184': 'Use as human shield',\n",
    "    '185': 'Attempt to assassinate',\n",
    "    '186': 'Assassinate',\n",
    "\n",
    "    '190': 'Use conventional military force, not specified below',\n",
    "    '191': 'Impose blockade, restrict movement',\n",
    "    '192': 'Occupy territory',\n",
    "    '193': 'Fight with small arms and light weapons',\n",
    "    '194': 'Fight with artillery and tanks',\n",
    "    '195': 'Employ aerial weapons, not specified below',\n",
    "    '1951': 'Employ precision-guided aerial munitions', \n",
    "    '1952': 'Employ remotely piloted aerial munitions', \n",
    "    '196': 'Violate ceasefire',\n",
    "\n",
    "    '200': 'Use unconventional mass violence, not specified below',\n",
    "    '201': 'Engage in mass expulsion',\n",
    "    '202': 'Engage in mass killings',\n",
    "    '203': 'Engage in ethnic cleansing',\n",
    "    '204': 'Use weapons of mass destruction, not specified below', \n",
    "    '2041': 'Use chemical, biological, or radiological weapons',  \n",
    "    '2042': 'Detonate nuclear weapons'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select period we are going to fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime('2022-07-01 00:00:00')\n",
    "end = pd.to_datetime('2022-10-01 00:00:00')\n",
    "\n",
    "# start = pd.to_datetime('2020-01-01 00:00:00')\n",
    "# end = pd.to_datetime('2022-01-20 09:00:00')\n",
    "\n",
    "path_to_gdelt = os.path.join(PATH_TO_DATA_ROOT_DIR, 'data_providers/gdelt')\n",
    "\n",
    "create_output_dir(path_to_gdelt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.1 GDELT v2 English\n",
    "---\n",
    "\n",
    "**It has the following structure:**\n",
    "- xxx.translation.export.CSV.zip\n",
    "- xxx.translation.mentions.CSV.zip\n",
    "- xxx.translation.gkg.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all links to be fetched from Gdelt v2 for a given period of time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "links_covering_period_en_gdelt_v2 = get_links_for_time_range(start=start, end=end, multilingua_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - All Gdelt links (3 feeds: events, mentions, knowledge graph): 711474\n",
    "    - How many records per each feed:{'gkg': 237143, 'export': 237138, 'mentions': 237138}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download data for 1 datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_events = links_covering_period_en_gdelt_v2['feed_type'] == 'export'\n",
    "\n",
    "events_v02_en_export_df_1, errors = \\\n",
    "    fetch_events_data(gdelt_v02_events_link=links_covering_period_en_gdelt_v2.loc[\n",
    "        mask_events, 'url'].sample(1, random_state=13378).iloc[0])\n",
    "\n",
    "events_v02_en_export_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path_to_gdelt = os.path.join(PATH_TO_DATA_ROOT_DIR, 'data_providers/gdelt')\n",
    "fn = f'events_v02_en_export_df_{str(start.date())}_{str(end.date())}.parquet'\n",
    "create_output_dir(path_to_gdelt)\n",
    "    \n",
    "re_fetch_data = False\n",
    "\n",
    "if not os.path.exists(os.path.join(path_to_gdelt, fn)) or re_fetch_data:\n",
    "    print(f\"Fetching GDELT data\")\n",
    "    mask_events = links_covering_period_en_gdelt_v2['feed_type'] == 'export'\n",
    "    temp = parallelize(data=links_covering_period_en_gdelt_v2.loc[mask_events, 'url'].to_list(),\n",
    "                       func=__parallel_fetch_events)\n",
    "\n",
    "    events_v02_en_export_df_list = []\n",
    "    links_with_errors_list = []\n",
    "\n",
    "    for l in temp:\n",
    "        events_v02_en_export_df_list.append(l[0])\n",
    "        links_with_errors_list.append(l[1])\n",
    "\n",
    "    events_v02_en_export_df = pd.concat(events_v02_en_export_df_list, ignore_index=True)   \n",
    "    links_with_errors_list = list(itertools.chain(*links_with_errors_list))\n",
    "\n",
    "    print(f\"Number of Gdelt events for time range [{start}: {end}]: {events_v02_en_export_df.shape[0]}\")\n",
    "    print(f\"Number of broken links: {len(links_with_errors_list)}\")\n",
    "\n",
    "    assert events_v02_en_export_df['globaleventid'].nunique() == events_v02_en_export_df.shape[0],\\\n",
    "        \"globaleventid is not unique\"\n",
    "    \n",
    "    # Saving to parquet    \n",
    "    print(f\"\\nSaving fetched GDELT data to {os.path.join(path_to_gdelt, fn)}\")\n",
    "    events_v02_en_export_df.to_parquet(path=os.path.join(path_to_gdelt, fn), engine='auto', compression='snappy')\n",
    "else:\n",
    "    print(f\"Reading GDELT data from {os.path.join(path_to_gdelt, fn)}\")\n",
    "    events_v02_en_export_df = pd.read_parquet(os.path.join(path_to_gdelt, fn),\n",
    "                                              engine='auto',\n",
    "                                              columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [15:47:35] INFO generic-utils: Splitting input data into 96 batches\n",
    "- [15:47:35] INFO generic-utils: Starting parallel processing using 96 CPU\n",
    "- Number of Gdelt events for time range [2020-01-01 00:00:00: 2022-01-20 09:00:00]: 86160966\n",
    "- Number of broken links: 1\n",
    "- CPU times: user 42.2 s, sys: 32.1 s, total: 1min 14s\n",
    "- Wall time: 2min 31s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "duplicates_sourceurl = events_v02_en_export_df.groupby('sourceurl').agg(\n",
    "    {'globaleventid': ['count'],\n",
    "     'actiongeo_countrycode': [lambda x: x.value_counts().to_dict(), set],\n",
    "     'eventcode': [lambda x: x.value_counts().to_dict(), set]}).reset_index()\n",
    "\n",
    "duplicates_sourceurl.columns = [\"_\".join(filter(lambda col: col, col)) for col in duplicates_sourceurl.columns.ravel()]\n",
    "duplicates_sourceurl.rename(columns={'globaleventid_count': 'count',\n",
    "                                     'actiongeo_countrycode_<lambda_0>': 'actiongeo_countrycode_top5',\n",
    "                                     'eventcode_<lambda_0>': 'eventcode_top5'}, inplace=True)\n",
    "\n",
    "duplicates_sourceurl = duplicates_sourceurl[duplicates_sourceurl['count'] > 1]\n",
    "duplicates_sourceurl = duplicates_sourceurl.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "duplicates_sourceurl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wall time: 7min 59s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_v02_en_export_df[\n",
    "    events_v02_en_export_df['sourceurl'] == duplicates_sourceurl['sourceurl'].iloc[0]].sort_values(\n",
    "    ['sourceurl', 'eventcode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global coverage of Gdelt events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = events_v02_en_export_df['actiongeo_countrycode_iso3'].value_counts().reset_index()\n",
    "count = count.rename(columns={'actiongeo_countrycode_iso3': 'n_records', 'index': 'actiongeo_countrycode_iso3'})\n",
    "\n",
    "fig = px.choropleth(count,\n",
    "                    locations='actiongeo_countrycode_iso3',\n",
    "                    color='n_records',\n",
    "                    hover_name='n_records',\n",
    "                    title=f'Countries covered in Gdelt v2 events table ({start.date()} - {end.date()})',\n",
    "                    projection=\"natural earth\",  # \"natural earth\", \"mercator\", \"orthographic\"\n",
    "                    range_color=[count['n_records'].quantile(0.25), count['n_records'].quantile(0.995)])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting CAMEO codes for Base Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes = events_v02_en_export_df[events_v02_en_export_df['eventrootcode'].isin([14, 18, 19, 20])]\n",
    "\n",
    "selected_cameo_codes['eventcode_str'] = selected_cameo_codes['eventcode'].astype(str).map(cameo_event_code)\n",
    "selected_cameo_codes['eventbasecode_str'] = selected_cameo_codes['eventbasecode'].astype(str).map(cameo_event_base_code)\n",
    "selected_cameo_codes['eventrootcode_str'] = selected_cameo_codes['eventrootcode'].astype(str).map(cameo_event_root_code)\n",
    "\n",
    "print(selected_cameo_codes.shape)\n",
    "selected_cameo_codes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "selected_cameo_codes_duplicates_sourceurl = selected_cameo_codes.groupby('sourceurl').agg(\n",
    "    {'globaleventid': ['count'],\n",
    "     'dateadded': ['min', 'max', set],\n",
    "     'actiongeo_countrycode': [lambda x: x.value_counts().to_dict(), set],\n",
    "     'eventbasecode_str': [lambda x: \" | \".join(set(x))],\n",
    "     'eventrootcode_str': [lambda x: \" | \".join(set(x))],\n",
    "     'eventcode': [lambda x: x.value_counts().to_dict(), set]}).reset_index()\n",
    "\n",
    "selected_cameo_codes_duplicates_sourceurl.columns = \\\n",
    "    [\"_\".join(filter(lambda col: col, col)) for col in selected_cameo_codes_duplicates_sourceurl.columns.ravel()]\n",
    "\n",
    "selected_cameo_codes_duplicates_sourceurl.rename(\n",
    "    columns={'globaleventid_count': 'n_records_with_same_url',\n",
    "             'actiongeo_countrycode_<lambda_0>': 'actiongeo_countrycode_top5',\n",
    "             'eventbasecode_str_<lambda>': 'unique_eventbasecode_str_for_same_url',\n",
    "             'eventrootcode_str_<lambda>': 'unique_eventrootcode_str_for_same_url',\n",
    "             'eventcode_<lambda_0>': 'eventcode_top5'}, inplace=True)\n",
    "\n",
    "selected_cameo_codes_duplicates_sourceurl = selected_cameo_codes_duplicates_sourceurl.sort_values(\n",
    "    'n_records_with_same_url', ascending=False).reset_index(drop=True)\n",
    "\n",
    "selected_cameo_codes_duplicates_sourceurl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes['eventrootcode_str'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes['eventbasecode_str'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes['eventcode_str'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset for Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes.groupby('sourceurl')['eventcode'].nunique().loc[lambda x: x > 1].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes_for_check = selected_cameo_codes.drop_duplicates('sourceurl').groupby(\n",
    "    ['eventcode']).sample(20, replace=True, random_state=1337)\n",
    "\n",
    "selected_cameo_codes_for_check = selected_cameo_codes_for_check.drop_duplicates(\n",
    "    subset=['eventcode', 'actiongeo_countrycode_iso2'])\n",
    "\n",
    "selected_cameo_codes_for_check = selected_cameo_codes_for_check.sort_values(\n",
    "    ['eventrootcode', 'dateadded']).reset_index(drop=True)\n",
    "\n",
    "selected_cameo_codes_for_check['actiongeo_country_name'] = \\\n",
    "    selected_cameo_codes_for_check['actiongeo_countrycode_iso2'].map(map_iso2_to_country_name)\n",
    "\n",
    "cols_of_interest = [\n",
    "    'globaleventid', 'dateadded', 'actiongeo_country_name', 'eventrootcode', 'eventbasecode', \n",
    "    'eventcode', 'eventrootcode_str', 'eventbasecode_str', 'eventcode_str', 'sourceurl',\n",
    "    # 'actiongeo_countrycode_iso2', \n",
    "]\n",
    "\n",
    "selected_cameo_codes_for_check = selected_cameo_codes_for_check[cols_of_interest]\n",
    "\n",
    "print(selected_cameo_codes_for_check.shape[0])\n",
    "selected_cameo_codes_for_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes_for_check['eventcode_str'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes_for_check['eventbasecode_str'].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes_for_check_final = selected_cameo_codes_for_check.merge(\n",
    "    selected_cameo_codes_duplicates_sourceurl[['sourceurl', 'n_records_with_same_url', 'actiongeo_countrycode_set',\n",
    "                                               'eventcode_set', 'unique_eventrootcode_str_for_same_url',\n",
    "                                               'unique_eventbasecode_str_for_same_url']],\n",
    "    how='left', on='sourceurl')\n",
    "\n",
    "print(selected_cameo_codes_for_check_final.shape[0])\n",
    "selected_cameo_codes_for_check_final.head()\n",
    "\n",
    "save_file = False\n",
    "\n",
    "if save_file:\n",
    "    selected_cameo_codes_for_check_final.to_csv('selected_cameo_codes_for_check_v0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cameo_codes_for_check_final.sort_values('n_records_with_same_url', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 GDELT v2 Multi-lingua\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all links to be fetched from Gdelt v2 for a given period of time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "links_covering_period_ml_gdelt_v2 = get_links_for_time_range(start=start, end=end, multilingua_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download data for 1 datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_events = links_covering_period_ml_gdelt_v2['feed_type'] == 'export'\n",
    "\n",
    "events_v02_ml_export_df_1, errors = \\\n",
    "    fetch_events_data(gdelt_v02_events_link=links_covering_period_ml_gdelt_v2.loc[mask_events, 'url'].sample(\n",
    "        1, random_state=13378).iloc[0])\n",
    "\n",
    "events_v02_ml_export_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path_to_gdelt = os.path.join(PATH_TO_DATA_ROOT_DIR, 'data_providers/gdelt')\n",
    "fn = f'events_v02_ml_export_df_{str(start.date())}_{str(end.date())}.parquet'\n",
    "create_output_dir(path_to_gdelt)\n",
    "    \n",
    "re_fetch_data = False\n",
    "\n",
    "if not os.path.exists(os.path.join(path_to_gdelt, fn)) or re_fetch_data:\n",
    "    print(f\"Fetching GDELT data\")\n",
    "    mask_events = links_covering_period_ml_gdelt_v2['feed_type'] == 'export'\n",
    "    temp = parallelize(data=links_covering_period_ml_gdelt_v2.loc[mask_events, 'url'].to_list(),\n",
    "                       func=__parallel_fetch_events)\n",
    "\n",
    "    events_v02_ml_export_df_list = []\n",
    "    links_with_errors_list = []\n",
    "\n",
    "    for l in temp:\n",
    "        events_v02_ml_export_df_list.append(l[0])\n",
    "        links_with_errors_list.append(l[1])\n",
    "\n",
    "    events_v02_ml_export_df = pd.concat(events_v02_ml_export_df_list, ignore_index=True)   \n",
    "    links_with_errors_list = list(itertools.chain(*links_with_errors_list))\n",
    "\n",
    "    print(f\"Number of Gdelt events for time range [{start}: {end}]: {events_v02_ml_export_df.shape[0]}\")\n",
    "    print(f\"Number of broken links: {len(links_with_errors_list)}\")\n",
    "\n",
    "    assert events_v02_ml_export_df['globaleventid'].nunique() == events_v02_ml_export_df.shape[0], \\\n",
    "        \"globaleventid is not unique\"\n",
    "    \n",
    "    # Saving to parquet    \n",
    "    print(f\"\\nSaving fetched GDELT data to {os.path.join(path_to_gdelt, fn)}\")\n",
    "    events_v02_ml_export_df.to_parquet(path=os.path.join(path_to_gdelt, fn), engine='auto', compression='snappy')\n",
    "else:\n",
    "    print(f\"Reading GDELT data from {os.path.join(path_to_gdelt, fn)}\")\n",
    "    events_v02_ml_export_df = pd.read_parquet(os.path.join(path_to_gdelt, fn),\n",
    "                                              engine='auto',\n",
    "                                              columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fetching GDELT data\n",
    "- [21:55:12] INFO generic-utils: Splitting input data into 96 batches\n",
    "- [21:55:12] INFO generic-utils: Starting parallel processing using 96 CPU\n",
    "- Number of Gdelt events for time range [2020-01-01 00:00:00: 2022-01-20 09:00:00]: 41909854\n",
    "- Number of broken links: 0\n",
    "- Saving fetched GDELT data to /home/sergii/workspace/data/data_providers/gdelt/events_v02_ml_export_df_2020-01-01_2022-01-20.parquet\n",
    "- CPU times: user 54.4 s, sys: 21.9 s, total: 1min 16s\n",
    "- Wall time: 2min 43s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data to RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "duplicates_sourceurl_ml = events_v02_ml_export_df.groupby('sourceurl').agg(\n",
    "    {'globaleventid': ['count'],\n",
    "     'actiongeo_countrycode': [lambda x: x.value_counts().to_dict(), set],\n",
    "     'eventcode': [lambda x: x.value_counts().to_dict(), set]}).reset_index()\n",
    "\n",
    "duplicates_sourceurl_ml.columns = \\\n",
    "    [\"_\".join(filter(lambda col: col, col)) for col in duplicates_sourceurl_ml.columns.ravel()]\n",
    "\n",
    "duplicates_sourceurl_ml.rename(columns={'globaleventid_count': 'count',\n",
    "                                        'actiongeo_countrycode_<lambda_0>': 'actiongeo_countrycode_top5',\n",
    "                                        'eventcode_<lambda_0>': 'eventcode_top5'}, inplace=True)\n",
    "\n",
    "duplicates_sourceurl_ml = duplicates_sourceurl_ml[duplicates_sourceurl_ml['count'] > 1]\n",
    "duplicates_sourceurl_ml = duplicates_sourceurl_ml.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "duplicates_sourceurl_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. GDELT v2 GKG (downloading 2020-2022)\n",
    "---\n",
    "\n",
    "Global Knowledge Graph (GKG) - is updated every 15 minutes and based on global news reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_themes_categories(x):\n",
    "    if isinstance(x, str):\n",
    "        if x:\n",
    "            return [c for c in sorted(set([c.split(',')[0] for c in x.split(';')])) if c]\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def extract_categories_from_count(x):\n",
    "    if isinstance(x, str):\n",
    "        return sorted(m for m in set([c.split('#')[0] if c else c for c in x.split(';')]) if m)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def fetch_gkg_data(gdelt_v02_gkg_link: str) -> pd.DataFrame:\n",
    "\n",
    "    use_cols_gkg = [\n",
    "        'GKGRECORDID', 'DATE', 'SourceCommonName', 'DocumentIdentifier', 'V2Counts', 'V2Themes',\n",
    "        'V2Locations', 'Dates', \n",
    "        'TranslationInfo'\n",
    "\n",
    "        # These we may want to use when training NER\n",
    "        # 'V2Persons', \n",
    "        # 'V2Organizations',\n",
    "        # 'SourceCollectionIdentifier', \n",
    "        # 'Quotations', \n",
    "        # 'AllNames', \n",
    "        # 'Amounts', \n",
    "    ]\n",
    "\n",
    "    cols_order = [\n",
    "        'gkgrecordid', 'date', 'sourcecommonname', 'documentidentifier', 'v2locations', \n",
    "        'v2themes_names', 'v2counts_names', 'dates', 'translationinfo'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        gkg_v02_df = pd.read_csv(gdelt_v02_gkg_link, sep='\\t', encoding = \"ISO-8859-1\", header=None, names=gkg_header)\n",
    "        gkg_v02_df = gkg_v02_df[use_cols_gkg]\n",
    "        gkg_v02_df.columns = gkg_v02_df.columns.str.lower()\n",
    "        gkg_v02_df['date'] = pd.to_datetime(gkg_v02_df['date'], format='%Y%m%d%H%M%S')\n",
    "        gkg_v02_df['v2themes_names'] = gkg_v02_df['v2themes'].apply(extract_themes_categories)\n",
    "        gkg_v02_df['v2counts_names'] = gkg_v02_df['v2counts'].apply(extract_categories_from_count)\n",
    "        gkg_v02_df = gkg_v02_df[cols_order]\n",
    "        return gkg_v02_df, ''\n",
    "\n",
    "    except:\n",
    "        return pd.DataFrame(), gdelt_v02_gkg_link\n",
    "    \n",
    "    \n",
    "def __parallel_fetch_gkg(gdelt_v02_gkg_link_list):\n",
    "    \n",
    "    gkg_v02_df_list = []\n",
    "    links_with_errors_list = []\n",
    "    \n",
    "    for l in gdelt_v02_gkg_link_list:\n",
    "        df_, links_with_errors = fetch_gkg_data(gdelt_v02_gkg_link=l)\n",
    "    \n",
    "        if not df_.empty:\n",
    "            gkg_v02_df_list.append(df_)\n",
    "\n",
    "        if links_with_errors:\n",
    "            links_with_errors_list.append(links_with_errors)\n",
    "        \n",
    "    gkg_v02_df = pd.concat(gkg_v02_df_list, ignore_index=True)\n",
    "    return gkg_v02_df, links_with_errors_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GDELT v2 GKG English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_covering_period_en_gdelt_v2 = get_links_for_time_range(start=start, end=end, multilingua_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_gkg = links_covering_period_en_gdelt_v2['feed_type'] == 'gkg'\n",
    "\n",
    "gkg_v02_en_gkg_df_1, errors = \\\n",
    "    fetch_gkg_data(gdelt_v02_gkg_link=links_covering_period_en_gdelt_v2.loc[mask_gkg, 'url'].sample(\n",
    "        1, random_state=3378).iloc[0])\n",
    "\n",
    "gkg_v02_en_gkg_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path_to_gdelt = os.path.join(PATH_TO_DATA_ROOT_DIR, 'data_providers/gdelt')\n",
    "fn = f'gkg_v02_en_df_{str(start.date())}_{str(end.date())}.parquet'\n",
    "create_output_dir(path_to_gdelt)\n",
    "    \n",
    "re_fetch_data = False\n",
    "\n",
    "if not os.path.exists(os.path.join(path_to_gdelt, fn)) or re_fetch_data:\n",
    "    print(f\"Fetching GDELT data\")\n",
    "    mask_gkg = links_covering_period_en_gdelt_v2['feed_type'] == 'gkg'\n",
    "    temp = parallelize(data=links_covering_period_en_gdelt_v2.loc[mask_gkg, 'url'].to_list(),\n",
    "                       func=__parallel_fetch_gkg)\n",
    "    \n",
    "    print(\"Extracting pairs from processed data\")\n",
    "    gkg_v02_en_df_list = []\n",
    "    links_with_errors_list = []\n",
    "\n",
    "    for l in temp:\n",
    "        gkg_v02_en_df_list.append(l[0])\n",
    "        links_with_errors_list.append(l[1])\n",
    "\n",
    "    print(\"Concatenating results\")\n",
    "    gkg_v02_en_df = pd.concat(gkg_v02_en_df_list, ignore_index=True)   \n",
    "    links_with_errors_list = list(itertools.chain(*links_with_errors_list))\n",
    "\n",
    "    print(f\"Number of Gdelt gkg records for time range [{start}: {end}]: {gkg_v02_en_df.shape[0]}\")\n",
    "    print(f\"Number of broken links: {len(links_with_errors_list)}\")\n",
    "\n",
    "    # assert gkg_v02_en_df['gkgrecordid'].nunique() == gkg_v02_en_df.shape[0], \"gkgrecordid is not unique\"\n",
    "    \n",
    "    # Saving to parquet    \n",
    "    print(f\"\\nSaving fetched GDELT GKG EN data to {os.path.join(path_to_gdelt, fn)}\")\n",
    "    gkg_v02_en_df.to_parquet(path=os.path.join(path_to_gdelt, fn), engine='auto', compression='snappy')\n",
    "else:\n",
    "    print(f\"Reading GDELT GKG EN data from {os.path.join(path_to_gdelt, fn)}\")\n",
    "    gkg_v02_en_df = pd.read_parquet(os.path.join(path_to_gdelt, fn),\n",
    "                                    engine='auto',\n",
    "                                    columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fetching GDELT data\n",
    "- [01:12:17] INFO generic-utils: Splitting input data into 96 batches\n",
    "- [01:12:17] INFO generic-utils: Starting parallel processing using 96 CPU\n",
    "- Extracting pairs from processed data\n",
    "- Concatenating results\n",
    "- Number of Gdelt gkg records for time range [2020-01-01 00:00:00: 2022-01-20 09:00:00]: 101022990\n",
    "- Number of broken links: 1\n",
    "- \n",
    "- Saving fetched GDELT GKG EN data to /home/sergii/workspace/data/data_providers/gdelt/gkg_v02_en_df_2020-01-01_2022-01-20.parquet\n",
    "- CPU times: user 27min 56s, sys: 14min 48s, total: 42min 45s\n",
    "- Wall time: 49min 32s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkg_v02_en_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkg_v02_en_df['documentidentifier'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkg_v02_en_df['sourcecommonname'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gkg_v02_en_df.head(100).explode(['v2themes_names']).value_counts(normalize=True).head(10).round(3)*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "largest_sources_gkg_v02_en = \\\n",
    "    (gkg_v02_en_df['sourcecommonname'].value_counts(normalize=True, dropna=False) * 100.).reset_index()\n",
    "\n",
    "largest_sources_gkg_v02_en.rename(columns={'sourcecommonname': 'pct_url_covered', 'index': 'sourcecommonname'},\n",
    "                                  inplace=True)\n",
    "\n",
    "largest_sources_gkg_v02_en['pct_url_covered_cumulative'] = largest_sources_gkg_v02_en['pct_url_covered'].cumsum()\n",
    "\n",
    "print(f\"Number of News sources covering 90% of all URLs (EN): \"\n",
    "      f\"{largest_sources_gkg_v02_en[largest_sources_gkg_v02_en['pct_url_covered_cumulative'] <= 90.0].shape[0]} \"\n",
    "      f\"/ {largest_sources_gkg_v02_en.shape[0]}\")\n",
    "\n",
    "print(f\"Number of News sources covering 95% of all URLs (EN): \"\n",
    "      f\"{largest_sources_gkg_v02_en[largest_sources_gkg_v02_en['pct_url_covered_cumulative'] <= 95.0].shape[0]} \"\n",
    "      f\"/ {largest_sources_gkg_v02_en.shape[0]}\")\n",
    "\n",
    "print(f\"Number of News sources covering 99% of all URLs (EN): \"\n",
    "      f\"{largest_sources_gkg_v02_en[largest_sources_gkg_v02_en['pct_url_covered_cumulative'] <= 99.0].shape[0]} \"\n",
    "      f\"/ {largest_sources_gkg_v02_en.shape[0]}\\n\")\n",
    "\n",
    "largest_sources_gkg_v02_en.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GDELT v2 GKG Multi-lingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_covering_period_ml_gdelt_v2 = get_links_for_time_range(start=start, end=end, multilingua_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_gkg = links_covering_period_ml_gdelt_v2['feed_type'] == 'gkg'\n",
    "\n",
    "gkg_v02_ml_gkg_df_1, errors = \\\n",
    "    fetch_gkg_data(gdelt_v02_gkg_link=links_covering_period_ml_gdelt_v2.loc[mask_gkg, 'url'].sample(\n",
    "        1, random_state=13378).iloc[0])\n",
    "\n",
    "gkg_v02_ml_gkg_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path_to_gdelt = os.path.join(PATH_TO_DATA_ROOT_DIR, 'data_providers/gdelt')\n",
    "fn = f'gkg_v02_ml_df_{str(start.date())}_{str(end.date())}.parquet'\n",
    "create_output_dir(path_to_gdelt)\n",
    "    \n",
    "re_fetch_data = True\n",
    "\n",
    "if not os.path.exists(os.path.join(path_to_gdelt, fn)) or re_fetch_data:\n",
    "    print(f\"Fetching GDELT data\")\n",
    "    mask_gkg = links_covering_period_ml_gdelt_v2['feed_type'] == 'gkg'\n",
    "    temp = parallelize(data=links_covering_period_ml_gdelt_v2.loc[mask_gkg, 'url'].to_list(),\n",
    "                       func=__parallel_fetch_gkg)\n",
    "\n",
    "    gkg_v02_ml_df_list = []\n",
    "    links_with_errors_list = []\n",
    "\n",
    "    for l in temp:\n",
    "        gkg_v02_ml_df_list.append(l[0])\n",
    "        links_with_errors_list.append(l[1])\n",
    "\n",
    "    gkg_v02_ml_df = pd.concat(gkg_v02_ml_df_list, ignore_index=True)   \n",
    "    links_with_errors_list = list(itertools.chain(*links_with_errors_list))\n",
    "\n",
    "    print(f\"Number of Gdelt gkg records for time range [{start}: {end}]: {gkg_v02_ml_df.shape[0]}\")\n",
    "    print(f\"Number of broken links: {len(links_with_errors_list)}\")\n",
    "\n",
    "    # assert gkg_v02_ml_df['gkgrecordid'].nunique() == gkg_v02_ml_df.shape[0], \"gkgrecordid is not unique\"\n",
    "    \n",
    "    # Saving to parquet    \n",
    "    print(f\"\\nSaving fetched GDELT GKG ML data to {os.path.join(path_to_gdelt, fn)}\")\n",
    "    gkg_v02_ml_df.to_parquet(path=os.path.join(path_to_gdelt, fn), engine='auto', compression='snappy')\n",
    "else:\n",
    "    print(f\"Reading GDELT GKG ML data from {os.path.join(path_to_gdelt, fn)}\")\n",
    "    gkg_v02_ml_df = pd.read_parquet(os.path.join(path_to_gdelt, fn),\n",
    "                                    engine='auto',\n",
    "                                    columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fetching GDELT data\n",
    "- [02:07:30] INFO generic-utils: Splitting input data into 96 batches\n",
    "- [02:07:31] INFO generic-utils: Starting parallel processing using 96 CPU\n",
    "- Number of Gdelt gkg records for time range [2020-01-01 00:00:00: 2022-01-20 09:00:00]: 164772020\n",
    "- Number of broken links: 0\n",
    "- \n",
    "- Saving fetched GDELT GKG ML data to /home/sergii/workspace/data/data_providers/gdelt/gkg_v02_ml_df_2020-01-01_2022-01-20.parquet\n",
    "- CPU times: user 41min 21s, sys: 23min 50s, total: 1h 5min 11s\n",
    "- Wall time: 1h 10min 43s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3 Gdelt V2 - 15 minutes update\n",
    "---\n",
    "\n",
    "**It has the following structure:**\n",
    "- xxx.translation.export.CSV.zip\n",
    "- xxx.translation.mentions.CSV.zip\n",
    "- xxx.translation.gkg.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_to_fetch_gdelt_v2_data(base_url):\n",
    "    html = requests.get(base_url)\n",
    "    htmlParse = BeautifulSoup(html.text, 'lxml')\n",
    "    htmlParse.find_all()\n",
    "\n",
    "    links = []\n",
    "    for link in htmlParse.get_text().split('\\n'):\n",
    "        if link and 'http://' in link:\n",
    "            links.append(link.split()[-1])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate.txt\"\n",
    "\n",
    "en_links = get_links_to_fetch_gdelt_v2_data(base_url=base_url)\n",
    "en_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_export_links = [l for l in en_links if 'export' in en_links]\n",
    "mentions = [l for l in en_links if 'mentions' in en_links]\n",
    "gkg = [l for l in en_links if 'gkg' in en_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.1 Event**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_v02_en_export_15min = 'http://data.gdeltproject.org/gdeltv2/20220118204500.export.CSV.zip'\n",
    "\n",
    "use_cols_en_export = [\n",
    "    'GLOBALEVENTID', 'SQLDATE', 'EventCode', 'EventBaseCode', 'EventRootCode',\n",
    "    'GoldsteinScale', 'ActionGeo_FullName', 'ActionGeo_CountryCode', \n",
    "    'ActionGeo_Lat', 'ActionGeo_Long', 'DATEADDED', 'SOURCEURL'\n",
    "]\n",
    "\n",
    "events_v02_en_export_df_15min = pd.read_csv(events_v02_en_export_15min, sep='\\t', encoding = \"ISO-8859-1\",\n",
    "                                            header=None, names=event_export_header)\n",
    "\n",
    "events_v02_en_export_df_15min = events_v02_en_export_df_15min[use_cols_en_export]\n",
    "events_v02_en_export_df_15min.columns = events_v02_en_export_df_15min.columns.str.lower()\n",
    "\n",
    "print(events_v02_en_export_df_15min.shape)\n",
    "events_v02_en_export_df_15min.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_v02_en_export_df_15min[events_v02_en_export_df_15min['eventrootcode'].isin([14, 18, 19, 20])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.2 GKG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_v02_en_gkg_15min = 'http://data.gdeltproject.org/gdeltv2/20220118204500.gkg.csv.zip'\n",
    "\n",
    "use_cols_en_gkg = [\n",
    "    'GKGRECORDID', 'DATE', 'SourceCommonName', 'SourceCollectionIdentifier', 'DocumentIdentifier', 'V2Counts', 'V2Themes',\n",
    "    'V2Locations', 'V2Persons', 'V2Organizations', 'Dates', 'Quotations', 'AllNames', 'Amounts', \n",
    "    'TranslationInfo'\n",
    "]\n",
    "\n",
    "events_v02_en_gkg_df_15min = pd.read_csv(events_v02_en_gkg_15min, sep='\\t', encoding = \"ISO-8859-1\",\n",
    "                                         header=None, names=gkg_header)\n",
    "\n",
    "# events_v02_en_gkg_df_15min = events_v02_en_gkg_df_15min[use_cols_en_gkg]\n",
    "# events_v02_en_gkg_df_15min.columns = events_v02_en_gkg_df_15min.columns.str.lower()\n",
    "\n",
    "# print(events_v02_en_gkg_df_15min.shape)\n",
    "events_v02_en_gkg_df_15min.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_v02_en_gkg_df_15min[['Counts', 'V2Counts']].head(1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_v02_en_gkg_df_15min[['Themes', 'V2Themes']].head(1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_v02_en_gkg_df_15min[['Persons', 'V2Persons']].head(1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_v02_en_gkg_df_15min[['Organizations', 'V2Organizations']].head(1).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.3 Events vs GKG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(events_v02_en_gkg_df_15min['documentidentifier']).intersection(set(events_v02_en_export_df_15min['sourceurl'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = events_v02_en_export_df_15min.merge(events_v02_en_gkg_df_15min,\n",
    "                                                how='left',\n",
    "                                                left_on='sourceurl',\n",
    "                                                right_on='documentidentifier')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['eventrootcode'].isin([18])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Translingual [60+ languages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate-translation.txt\"\n",
    "\n",
    "ml_links = get_links_to_fetch_gdelt_v2_data(base_url=base_url)\n",
    "ml_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_v02_multilingua_export = 'http://data.gdeltproject.org/gdeltv2/20220118181500.translation.export.CSV.zip'\n",
    "\n",
    "events_v02_multilingua_export_df = pd.read_csv(events_v02_multilingua_export, sep='\\t', encoding = \"ISO-8859-1\",\n",
    "                                               header=None, names=event_export_header)\n",
    "\n",
    "events_v02_multilingua_export_df.columns = events_v02_multilingua_export_df.columns.str.lower()\n",
    "print(events_v02_multilingua_export_df.shape)\n",
    "events_v02_multilingua_export_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_v02_multilingua_export_df[events_v02_multilingua_export_df['eventrootcode'].isin([14, 18, 19, 20])]\n",
    "events_v02_multilingua_export_df[events_v02_multilingua_export_df['eventrootcode'].isin([19])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_v02_multilingua_gkg = 'http://data.gdeltproject.org/gdeltv2/20220118181500.translation.gkg.csv.zip'\n",
    "\n",
    "events_v02_multilingua_gkg_df = pd.read_csv(events_v02_multilingua_gkg, sep='\\t', encoding = \"ISO-8859-1\",\n",
    "                                            header=None, names=gkg_header)\n",
    "\n",
    "print(events_v02_multilingua_gkg_df.shape)\n",
    "events_v02_multilingua_gkg_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/tmp/20220118180000.gkg.csv'\n",
    "gdelt = pd.read_csv(filename, sep='\\t', encoding = \"ISO-8859-1\", header=None)\n",
    "gdelt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 How it was implemented by Jad (Optional cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down():\n",
    "    \"\"\"This function scrapes the GDELT website and get the latest 15 min update file\"\"\"\n",
    "    base_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate.txt\"\n",
    "    html = requests.get(base_url)\n",
    "    htmlParse = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "    gkg_download_link = htmlParse.find_all(\"p\")[0].get_text().split('\\n')[2].split(' ')[-1]\n",
    "    zip_link = gkg_download_link\n",
    "    response = requests.get(zip_link, stream=True)\n",
    "    place = '/tmp/gdelt_15_min.zip'\n",
    "    f = open(place, \"wb+\")\n",
    "    for chunk in response.iter_content(chunk_size=512):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "    f.close()\n",
    "    with zipfile.ZipFile(place, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = down()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38dnn] *",
   "language": "python",
   "name": "conda-env-py38dnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
